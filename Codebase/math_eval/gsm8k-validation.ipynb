{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset\nfrom huggingface_hub import login\nlogin(token=hf_token)\ndataset = load_dataset(\"lighteval/MATH\", split=\"test\", trust_remote_code = True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def formatting_prompts_func(example):\n    messages = [\n    {\"role\": \"user\", \"content\": f\"Problem: {example['problem']}\\n\\nSolve this math problem step by step.\"},\n    {\"role\": \"assistant\", \"content\": example['solution']},\n    ]\n    texts = tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = False)\n    return {\"task\": texts}\n\ndataset = dataset.map(formatting_prompts_func, batched=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tokenize_function(example):\n    outputs = tokenizer(example[\"task\"], truncation=True, padding=\"max_length\", max_length=5096)\n    return outputs\n    \ntokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"problem\", \"solution\", \"level\", \"type\"], load_from_cache_file=False)\ntokenized_datasets.set_format(\"torch\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth.chat_templates import get_chat_template\nfrom unsloth import FastLanguageModel\n\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"llama-3.1\",\n)\n\noutput_file = \"llama1b_r32_gsm8k_responses.jsonl\"\n#output_file = \"llama1b_r64_gsm8k_responses.jsonl\"\n#output_file = \"llama1b_r128_gsm8k_responses.jsonl\"\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = 'bharati2324/Llama-1B-Math-LoRA-r32-merged',\n    #model_name = 'bharati2324/Llama-1B-Math-LoRA-r64-merged',\n    #model_name = 'bharati2324/Llama-1B-Math-LoRA-r128-merged',\n    max_seq_length = 2048,\n    dtype = torch.float16,\n    load_in_4bit = False,\n)\nFastLanguageModel.for_inference(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_responses(problem_text, num_responses=5):\n    responses = []\n    input_prompt = [\n        {\"role\": \"user\", \"content\": f\"Q: {problem_text}\\n\\nSolve this math problem step by step.\\nA:\"\n     ]\n    input_ids = tokenizer.apply_chat_template(input_prompt, return_tensors=\"pt\").to(model.device)\n\n    for _ in range(num_responses):\n        output = model.generate(\n            input_ids=input_ids,\n            max_length=256,\n            num_return_sequences=1,\n            temperature=0.7,\n            top_p=0.9,\n            do_sample=True\n        )\n        response = tokenizer.decode(output[0], skip_special_tokens=True).split(\"A:\")[-1].strip()\n        responses.append(response)\n    return responses","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(output_file, \"w\") as f:\n    for idx, example in enumerate(dataset):\n        problem = example[\"question\"]\n        responses = generate_responses(problem, num_responses)\n        \n        output_entry = {\n            \"problem_id\": idx,\n            \"problem\": problem,\n            \"responses\": responses\n        }\n        f.write(json.dumps(output_entry) + \"\\n\")\n        print(f\"Processed problem {idx+1}/{len(dataset)}\")\n\nprint(f\"Responses saved to {output_file}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}