{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9703090,"sourceType":"datasetVersion","datasetId":5933917},{"sourceId":9705589,"sourceType":"datasetVersion","datasetId":5935841},{"sourceId":9705684,"sourceType":"datasetVersion","datasetId":5935912}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/openai/human-eval\n!pip install -e human-eval","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom human_eval.data import write_jsonl, read_problems\n\n# Set the device to GPU (CUDA) if available, otherwise use CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the Mistral-7B-Instruct-v0.1 model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\").to(device)\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\nmodel.eval()\ni = 0 ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_and_trim_code(text: str):\n    # Find the index where \"def \" starts\n    def_index = text.find('def ')\n    if def_index == -1:\n        return \"\"  # return an empty string if \"def \" is not found\n\n    # Extract the code starting from \"def \"\n    code_after_def = text[def_index:]\n\n    # Find the end of the \"def \" line\n    def_line_end_index = code_after_def.find('\\n')\n    if def_line_end_index != -1:\n        # Skip the entire line where \"def \" is found\n        code_after_def = code_after_def[def_line_end_index+1:]\n    \n    # Find the index of \"```\" that comes after \"def \"\n    end_code_index = code_after_def.find('```')\n    \n    # If \"```\" is found, return the code until just before \"```\"\n    if end_code_index != -1:\n        code = code_after_def[:end_code_index]\n    else:\n        # If no \"```\" is found, return the code until the end of the string\n        code = code_after_def\n    \n    # Removing leading and trailing whitespaces and newlines\n    return code.strip()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a function to generate one completion\ndef generate_one_completion(prompt):\n    global i \n    i += 1\n    print(\"Generating\")\n    print(i)\n\n    # Wrapping the prompt with instruction format tokens\n    formatted_prompt = f\"<s>[INST] You are a friendly chatbot. WRITE THE FULL COMPLETE FUNCTION (EG WITH def ....) END CODE WITH '```'. NOTE YOU ABSOLUTELY MUST END THE CODE WITH END CODE WITH '```' OR ELSE THE CODE WILL NOT BE INTERPRETTED!!!! {prompt} [/INST]\"\n\n    # Generate a response using the model\n    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n    outputs = None \n    with torch.no_grad():\n        outputs = model.generate(**inputs, max_new_tokens=512, do_sample=True, temperature=0.2, top_k=50, top_p=0.95, pad_token_id=tokenizer.eos_token_id)\n    \n    # Decode the generated ids to text\n    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Print the text split and trimmed\n    print(\"###\" * 50)\n    print(split_and_trim_code(text.split(\"[/INST]\")[1]))\n    return split_and_trim_code(text.split(\"[/INST]\")[1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read problems from the dataset\nproblems = read_problems()\n\n# Define the number of samples to generate for each problem\nnum_samples_per_task = 10\n\n# Generate samples\nsamples = [\n    dict(task_id=task_id, completion=generate_one_completion(problem[\"prompt\"]))\n    for task_id, problem in problems.items()\n    for _ in range(num_samples_per_task)\n]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"write_jsonl(\"mistral_samples.jsonl\", samples)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}