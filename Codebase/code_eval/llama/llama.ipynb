{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/openai/human-eval.git\n!pip install -e human-eval\nimport sys\nsys.path.append('/kaggle/working/human-eval')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import pipeline\nfrom human_eval.data import write_jsonl, read_problems\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nfrom huggingface_hub import login\n\n# Paste your Hugging Face token here\ntoken = \"\"\n\nlogin(token)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\npipe = pipeline(\n    \"text-generation\",\n    model=model_id,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i=0","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_and_trim_code(text: str):\n    # Find the index where \"def \" starts\n    def_index = text.find('def ')\n    if def_index == -1:\n        return \"\"  # return an empty string if \"def \" is not found\n\n    # Extract the code starting from \"def \"\n    code_after_def = text[def_index:]\n\n    # Find the end of the \"def \" line\n    def_line_end_index = code_after_def.find('\\n')\n    if def_line_end_index != -1:\n        # Skip the entire line where \"def \" is found\n        code_after_def = code_after_def[def_line_end_index+1:]\n    \n    # Find the index of \"```\" that comes after \"def \"\n    end_code_index = code_after_def.find('```')\n    \n    # If \"```\" is found, return the code until just before \"```\"\n    if end_code_index != -1:\n        code = code_after_def[:end_code_index]\n    else:\n        # If no \"```\" is found, return the code until the end of the string\n        code = code_after_def\n    \n    # Removing leading and trailing whitespaces and newlines\n    return code.strip()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_one_completion(prompt):\n    global i \n    i += 1\n    print(\"Generating\")\n    print(i)\n\n    # Wrapping the prompt with instruction format tokens\n    p = f\"WRITE THE FULL COMPLETE FUNCTION (EG WITH def ....) END CODE WITH '```'. NOTE YOU ABSOLUTELY MUST END THE CODE WITH END CODE WITH '```' OR ELSE THE CODE WILL NOT BE INTERPRETTED!!!! {prompt}\"\n\n    # Generate a response using the model\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a friendly chatbot\"},\n        {\"role\": \"user\", \"content\": p}\n    ]\n    \n    with torch.no_grad():\n        outputs = pipe(\n            messages,\n            max_new_tokens=3500,\n        )\n    \n    response = outputs[0][\"generated_text\"][-1]\n    \n    # Print the text split and trimmed\n    print(\"###\" * 10)\n    print(split_and_trim_code(response['content']))\n    return split_and_trim_code(response['content'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"problems = read_problems()\n\n# Define the number of samples to generate for each problem\nnum_samples_per_task = 10\n\n# Generate samples\nsamples = [\n    dict(task_id=task_id, completion=generate_one_completion(problem[\"prompt\"]))\n    for task_id, problem in problems.items()\n    for _ in range(num_samples_per_task)\n]\n\n# Save generated samples in jsonl format\nwrite_jsonl(\"/kaggle/working/llama_samples_10.jsonl\", samples)","metadata":{},"execution_count":null,"outputs":[]}]}
